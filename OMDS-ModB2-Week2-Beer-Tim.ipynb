{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5190b54c-d49c-4d6f-80cc-22555336a9cd",
   "metadata": {},
   "source": [
    "# Week 2 - Preprocessing, part 2\n",
    "\n",
    "# 1. Lesson: None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c4e5ff-b05f-4ef2-96f1-49dcb5beb158",
   "metadata": {},
   "source": [
    "# 2. Weekly graph question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad37e29-6e84-41fa-886d-abc1312213ab",
   "metadata": {},
   "source": [
    "The Storytelling With Data book mentions planning on a \"Who, What, and How\" for your data story.  Write down a possible Who, What, and How for your data, using the ideas in the book."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6b3c68",
   "metadata": {},
   "source": [
    "**Since I work in Network Management at Swiss Int. Air Lines, I will write an example from the perspective of my day-to-day work:**\n",
    "\n",
    "**Who**  \n",
    "The target audience for my data story is **stakeholders in the airline industry**, particularly **network planners and scheduling teams** at **SWISS International Air Lines**. They have a strong understanding of aviation operations but may need insights into market trends and performance metrics.  \n",
    "\n",
    "**What**  \n",
    "I want my audience to understand **how recent scheduling and network decisions have impacted operational efficiency and profitability**. The goal is to **highlight trends in passenger demand, route profitability, and slot utilization**, enabling decision-makers to optimize future scheduling.  \n",
    "\n",
    "**How**  \n",
    "I will communicate my insights through **a concise data visualization report**, supplemented with **interactive charts in Python using Matplotlib and Pandas**. This report will be shared via **email and a live presentation**, allowing for deeper discussion and quick decision-making.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898eb327-aefd-4ac0-b95a-92b616a2181b",
   "metadata": {},
   "source": [
    "# 3. Homework - work with your own data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe925521-979f-4983-8d85-8db8d1316e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14836788-b235-4cd4-b94d-5f749c6141a8",
   "metadata": {},
   "source": [
    "This week, you will do the same types of exercises as last week, but you should use your own datasets that you found last semester.\n",
    "\n",
    "### Here are some types of analysis you can do  Use Google, documentation, and ChatGPT to help you:\n",
    "\n",
    "- Summarize the datasets using info() and describe()\n",
    "\n",
    "- Are there any duplicate rows?\n",
    "\n",
    "- Are there any duplicate values in a given column (when this would be inappropriate?)\n",
    "\n",
    "- What are the mean, median, and mode of each column?\n",
    "\n",
    "- Are there any missing or null values?\n",
    "\n",
    "    - Do you want to fill in the missing value with a mean value?  A value of your choice?  Remove that row?\n",
    "\n",
    "- Identify any other inconsistent data (e.g. someone seems to be taking an action before they are born.)\n",
    "\n",
    "- Encode any categorical variables (e.g. with one-hot encoding.)\n",
    "\n",
    "### Conclusions:\n",
    "\n",
    "- Are the data usable?  If not, find some new data!\n",
    "\n",
    "- Do you need to modify or correct the data in some way?\n",
    "\n",
    "- Is there any class imbalance?  (Categories that have many more items than other categories)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce76e96b",
   "metadata": {},
   "source": [
    "**1. Information about the dataset I chose**\n",
    "\n",
    "The **Bank Customer Segmentation** dataset on Kaggle provides a collection of customer demographics and transaction data from an Indian bank. This dataset includes over a million transactions, capturing details such as customer age, gender, location, account balance, and transaction amounts. By analyzing these variables, one could identify distinct customer segments and understand their unique behaviors and preferences. This segmentation is crucial for financial institutions aiming to tailor their services effectively, enhance customer satisfaction, and reduce the chance of customer churning. The dataset's extensive scope offers valuable insights into customer interactions, enabling banks to develop targeted approaches that meet the specific needs of different client groups and foster stronger customer relationships.\n",
    "\n",
    "Link:\n",
    "https://www.kaggle.com/datasets/shivamb/bank-customer-segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c069e55",
   "metadata": {},
   "source": [
    "**2. Summarize the dataset using info() and describe()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed105e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1048567 entries, 0 to 1048566\n",
      "Data columns (total 9 columns):\n",
      " #   Column                   Non-Null Count    Dtype  \n",
      "---  ------                   --------------    -----  \n",
      " 0   TransactionID            1048567 non-null  object \n",
      " 1   CustomerID               1048567 non-null  object \n",
      " 2   CustomerDOB              1045170 non-null  object \n",
      " 3   CustGender               1047467 non-null  object \n",
      " 4   CustLocation             1048416 non-null  object \n",
      " 5   CustAccountBalance       1046198 non-null  float64\n",
      " 6   TransactionDate          1048567 non-null  object \n",
      " 7   TransactionTime          1048567 non-null  int64  \n",
      " 8   TransactionAmount (INR)  1048567 non-null  float64\n",
      "dtypes: float64(2), int64(1), object(6)\n",
      "memory usage: 72.0+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CustAccountBalance</th>\n",
       "      <th>TransactionTime</th>\n",
       "      <th>TransactionAmount (INR)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.046198e+06</td>\n",
       "      <td>1.048567e+06</td>\n",
       "      <td>1.048567e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.154035e+05</td>\n",
       "      <td>1.570875e+05</td>\n",
       "      <td>1.574335e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.464854e+05</td>\n",
       "      <td>5.126185e+04</td>\n",
       "      <td>6.574743e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.721760e+03</td>\n",
       "      <td>1.240300e+05</td>\n",
       "      <td>1.610000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.679218e+04</td>\n",
       "      <td>1.642260e+05</td>\n",
       "      <td>4.590300e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.765736e+04</td>\n",
       "      <td>2.000100e+05</td>\n",
       "      <td>1.200000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.150355e+08</td>\n",
       "      <td>2.359590e+05</td>\n",
       "      <td>1.560035e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       CustAccountBalance  TransactionTime  TransactionAmount (INR)\n",
       "count        1.046198e+06     1.048567e+06             1.048567e+06\n",
       "mean         1.154035e+05     1.570875e+05             1.574335e+03\n",
       "std          8.464854e+05     5.126185e+04             6.574743e+03\n",
       "min          0.000000e+00     0.000000e+00             0.000000e+00\n",
       "25%          4.721760e+03     1.240300e+05             1.610000e+02\n",
       "50%          1.679218e+04     1.642260e+05             4.590300e+02\n",
       "75%          5.765736e+04     2.000100e+05             1.200000e+03\n",
       "max          1.150355e+08     2.359590e+05             1.560035e+06"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(\"week2_bank_transactions.csv\")\n",
    "\n",
    "# Display dataset info\n",
    "df.info()\n",
    "\n",
    "# Display summary statistics\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1409405f",
   "metadata": {},
   "source": [
    "**3. Are there any duplicate rows?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6f16a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows: 0\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicate rows\n",
    "duplicate_rows = df.duplicated().sum()\n",
    "\n",
    "# Print the result\n",
    "print(f\"Number of duplicate rows: {duplicate_rows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb39cedb",
   "metadata": {},
   "source": [
    "**4.Are there any duplicate values in a given column?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97486429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate values in 'CustomerID': 164302\n"
     ]
    }
   ],
   "source": [
    "# I will specifically check for duplicates in the \"CustomerID\" column as it is critical that each value is unique.\n",
    "\n",
    "# Specify the column to check for duplicates\n",
    "column_name = \"CustomerID\"  # Change this to any column you want to check\n",
    "\n",
    "# Count duplicate values in the specified column\n",
    "duplicate_values = df[column_name].duplicated().sum()\n",
    "\n",
    "# Print the result\n",
    "print(f\"Number of duplicate values in '{column_name}': {duplicate_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb703b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of rows: 1048567\n",
      "Number of rows after removing duplicates: 884265\n"
     ]
    }
   ],
   "source": [
    "# Since there are some duplicates I will need to drop some rows.\n",
    "\n",
    "# Drop duplicate rows based on 'CustomerID' and keep the first occurrence\n",
    "df_unique = df.drop_duplicates(subset=['CustomerID'], keep='first')\n",
    "\n",
    "# Print the number of rows before and after removing duplicates\n",
    "print(f\"Original number of rows: {len(df)}\")\n",
    "print(f\"Number of rows after removing duplicates: {len(df_unique)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccffecbc",
   "metadata": {},
   "source": [
    "**5. What are the mean, median, and mode of each column?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd0eb634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  Mean     Median      Mode\n",
      "CustAccountBalance       114824.517201   16780.16       0.0\n",
      "TransactionTime          157105.772589  164217.00  195708.0\n",
      "TransactionAmount (INR)    1578.020192     460.00     100.0\n"
     ]
    }
   ],
   "source": [
    "# Calculate mean, median, and mode for each numerical column\n",
    "mean_values = df_unique.select_dtypes(include=['number']).mean()\n",
    "median_values = df_unique.select_dtypes(include=['number']).median()\n",
    "\n",
    "# Since pandas' mode returns multiple values in some cases, we ensure we take only the first mode value\n",
    "mode_values = df_unique.select_dtypes(include=['number']).mode().iloc[0]\n",
    "\n",
    "# Display the results\n",
    "summary_stats = pd.DataFrame({\n",
    "    'Mean': mean_values,\n",
    "    'Median': median_values,\n",
    "    'Mode': mode_values\n",
    "})\n",
    "\n",
    "# Print the summary statistics\n",
    "print(summary_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5e17f5",
   "metadata": {},
   "source": [
    "**6.Are there any missing or null values?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f4554f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in each column:\n",
      "CustomerDOB           2834\n",
      "CustGender             937\n",
      "CustLocation           134\n",
      "CustAccountBalance    2018\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "missing_values = df_unique.isnull().sum()\n",
    "\n",
    "# Print only columns that have missing values\n",
    "missing_values = missing_values[missing_values > 0]\n",
    "\n",
    "# Display the result\n",
    "print(\"Missing values in each column:\")\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8997f6",
   "metadata": {},
   "source": [
    "To have a clean dataset for any kind of further analysis, I will drop rows where CustomerDOB, CustGender, or CustLocation is missing or zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "24f88812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows before cleaning: 884265\n",
      "Number of rows after cleaning: 880412\n"
     ]
    }
   ],
   "source": [
    "# Drop rows where CustomerDOB, CustGender, or CustLocation is missing or zero\n",
    "df_cleaned = df_unique[\n",
    "    (df_unique['CustomerDOB'].notnull()) &\n",
    "    (df_unique['CustGender'].notnull()) &\n",
    "    (df_unique['CustLocation'].notnull()) &\n",
    "    (df_unique['CustomerDOB'] != 0) &\n",
    "    (df_unique['CustGender'] != 0) &\n",
    "    (df_unique['CustLocation'] != 0)\n",
    "]\n",
    "\n",
    "# Print the number of rows before and after cleaning\n",
    "print(f\"Number of rows before cleaning: {len(df_unique)}\")\n",
    "print(f\"Number of rows after cleaning: {len(df_cleaned)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0352fa1c",
   "metadata": {},
   "source": [
    "**7. Identify any other inconsistent data (e.g. someone seems to be taking an action before they are born)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eafe7da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15702/3457150455.py:4: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df_cleaned['CustomerDOB'] = pd.to_datetime(df_cleaned['CustomerDOB'], dayfirst=True, errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unrealistic CustomerDOB values: 136217\n",
      "        CustomerID CustomerDOB\n",
      "1         C2142763  2057-04-04\n",
      "3         C5342380  2073-09-14\n",
      "5         C1536588  2072-10-08\n",
      "15        C8334633  2068-07-10\n",
      "16        C1376215  1800-01-01\n",
      "...            ...         ...\n",
      "1048500   C1619019  2051-07-01\n",
      "1048504   C3321544  2070-12-18\n",
      "1048510   C3429427  2048-07-15\n",
      "1048531   C6937679  1800-01-01\n",
      "1048552   C5091732  2071-08-21\n",
      "\n",
      "[136217 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15702/3457150455.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned['CustomerDOB'] = pd.to_datetime(df_cleaned['CustomerDOB'], dayfirst=True, errors='coerce')\n"
     ]
    }
   ],
   "source": [
    "# I want to check, if all the birth dates are reasonable.\n",
    "\n",
    "# Attempt automatic date parsing with multiple formats\n",
    "df_cleaned['CustomerDOB'] = pd.to_datetime(df_cleaned['CustomerDOB'], dayfirst=True, errors='coerce')\n",
    "\n",
    "# Define realistic date range\n",
    "min_birth_date = datetime(1920, 1, 1)  # Assuming no one is older than 104 years\n",
    "max_birth_date = datetime(2024, 1, 1)  # No future birthdates\n",
    "\n",
    "# Find unrealistic birthdates again\n",
    "unrealistic_dobs = df_cleaned[(df_cleaned['CustomerDOB'] < min_birth_date) | (df_cleaned['CustomerDOB'] > max_birth_date)]\n",
    "\n",
    "# Print results\n",
    "print(f\"Number of unrealistic CustomerDOB values: {len(unrealistic_dobs)}\")\n",
    "print(unrealistic_dobs[['CustomerID', 'CustomerDOB']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eec15162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows after dropping unrealistic birth dates: 744195\n"
     ]
    }
   ],
   "source": [
    "# I want to drop all the rows with unrealistic birth dates\n",
    "\n",
    "# Keep only rows with realistic CustomerDOB values\n",
    "df_cleaned = df_cleaned[(df_cleaned['CustomerDOB'] >= min_birth_date) & (df_cleaned['CustomerDOB'] <= max_birth_date)]\n",
    "\n",
    "# Print the number of rows after cleaning\n",
    "print(f\"Number of rows after dropping unrealistic birth dates: {len(df_cleaned)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84371718",
   "metadata": {},
   "source": [
    "**8. Encode any categorical variables (e.g. with one-hot encoding.)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1de4c260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  TransactionID CustomerID CustomerDOB CustLocation  CustAccountBalance  \\\n",
      "0            T1   C5841053  1994-01-10   JAMSHEDPUR            17819.05   \n",
      "2            T3   C4417068  1996-11-26       MUMBAI            17874.44   \n",
      "4            T5   C9031234  1988-03-24  NAVI MUMBAI             6714.43   \n",
      "6            T7   C7126560  1992-01-26       MUMBAI              973.46   \n",
      "7            T8   C1220223  1982-01-27       MUMBAI            95075.54   \n",
      "\n",
      "  TransactionDate  TransactionTime  TransactionAmount (INR)  CustGender_M  \n",
      "0          2/8/16           143207                     25.0         False  \n",
      "2          2/8/16           142712                    459.0         False  \n",
      "4          2/8/16           181156                   1762.5         False  \n",
      "6          2/8/16           173806                    566.0         False  \n",
      "7          2/8/16           170537                    148.0          True  \n"
     ]
    }
   ],
   "source": [
    "# I think one specific column which makes sense to encode would be the gender column\n",
    "\n",
    "# Perform one-hot encoding on CustGender and drop the original column\n",
    "df_cleaned_encoded = pd.get_dummies(df_cleaned, columns=['CustGender'], drop_first=True)\n",
    "\n",
    "# Print the first few rows to verify the encoding\n",
    "print(df_cleaned_encoded.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6918583a",
   "metadata": {},
   "source": [
    "**9. Conclusion**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ce4d41",
   "metadata": {},
   "source": [
    "**1. Are the data usable?**\n",
    "Yes, but some preprocessing was necessary.  \n",
    "- The dataset contains meaningful transaction and customer information.\n",
    "- I handled missing values and removed unrealistic birthdates.\n",
    "- I encoded categorical variables (`CustGender` and `CustLocation`).\n",
    "- There were duplicate entries, which I resolved.\n",
    "\n",
    "\n",
    "\n",
    "**2. Do we need to modify or correct the data further?**\n",
    "Minor fixes were needed, but the dataset is now clean.  \n",
    "- **Date Format Issue:** The `CustomerDOB` column had inconsistent formats, but I fixed this.\n",
    "- **Missing Values:** I dropped rows where critical fields (`CustomerDOB`, `CustGender`, `CustLocation`) were missing.\n",
    "- **Duplicate Data:** Duplicates were removed based on `CustomerID`.\n",
    "\n",
    "\n",
    "\n",
    "**3. Is there any class imbalance?**\n",
    "Potential class imbalance in categorical features.  \n",
    "- **`CustGender`**: If one gender significantly outnumbers the other, it might impact certain analyses.\n",
    "- **`CustLocation`**: Some locations may dominate the dataset, leading to a bias in regional insights.\n",
    "- **`TransactionAmount (INR)` Distribution:** If most transactions are small amounts but a few outliers are significantly high, this could skew analysis.\n",
    "\n",
    "**Next Steps:**  \n",
    "- Check the distribution of `CustGender` using `df_cleaned['CustGender'].value_counts()`.\n",
    "- Check `CustLocation` imbalance using `df_cleaned['CustLocation'].value_counts()`.\n",
    "- Visualize `TransactionAmount (INR)` distribution to spot any outliers.\n",
    "\n",
    "\n",
    "**Final Verdict**\n",
    "The dataset is now clean and usable for analysis.  \n",
    "One should be aware of potential class imbalances before making conclusions based on customer demographics or transaction patterns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abab9e6d-18cc-4863-b980-3e52f581763a",
   "metadata": {},
   "source": [
    "# 4. Storytelling With Data graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1911148d-9df6-4b33-a875-8c96408ec834",
   "metadata": {},
   "source": [
    "Just like last week: choose any graph in the Introduction of Storytelling With Data. Use matplotlib to reproduce it in a rough way. I don't expect you to spend an enormous amount of time on this; I understand that you likely will not have time to re-create every feature of the graph. However, if you're excited about learning to use matplotlib, this is a good way to do that. You don't have to duplicate the exact values on the graph; just the same rough shape will be enough.  If you don't feel comfortable using matplotlib yet, do the best you can and write down what you tried or what Google searches you did to find the answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2888f9-3700-45ab-9829-6a5372106f9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
